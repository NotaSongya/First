{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48c07bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def remove_adjacent_duplicates(lines):\n",
    "    new_lines = []\n",
    "    for line in lines.split('\\n'):  # 按行分割多行数据\n",
    "        words = line.split()  # 将一行词汇拆分成单个单词\n",
    "        if len(words) == 0:  # 跳过空行\n",
    "            continue\n",
    "        new_words = [words[0]]  # 初始化去重后的单词列表，第一个单词不需要去重\n",
    "        for word in words[1:]:\n",
    "            if word != new_words[-1]:  # 如果当前单词与前一个不同，则加入去重后的单词列表\n",
    "                new_words.append(word)\n",
    "        new_lines.append(' '.join(new_words))  # 将去重后的单词列表转换为一行文本，添加到新的行列表中\n",
    "    return '\\n'.join(new_lines)\n",
    "\n",
    "# 打开文件，读取数据\n",
    "with open('C:\\\\Users\\\\FullZero\\\\Desktop\\\\fxxk\\\\malware_api_class-master\\\\mal-api-2019\\\\all_analysis_data.txt', 'r') as f:\n",
    "    lines = f.read()\n",
    "\n",
    "# 进行去重操作\n",
    "new_lines = remove_adjacent_duplicates(lines)\n",
    "\n",
    "# 将去重后的数据写回到新文件\n",
    "with open('C:\\\\Users\\\\FullZero\\\\Desktop\\\\output.txt', 'w') as f:\n",
    "    f.write(new_lines)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfa680f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# 打开输入文件并读取所有行\n",
    "with open('C:\\\\Users\\\\FullZero\\\\Desktop\\\\output.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# 将所有单词添加到集合中进行去重操作\n",
    "unique_words = set()\n",
    "for line in lines:\n",
    "    words = line.split()\n",
    "    unique_words.update(words)\n",
    "\n",
    "# 打开输出文件并将每个单词写入单独的行中\n",
    "with open('C:\\\\Users\\\\FullZero\\\\Desktop\\\\output2.txt', 'w') as f:\n",
    "    for word in unique_words:\n",
    "        f.write(word + '\\n')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "369db512",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# 读取文件并去重\n",
    "with open('C:\\\\Users\\\\FullZero\\\\Desktop\\\\output.txt', 'r') as f:\n",
    "    words = set(f.read().split())\n",
    "\n",
    "# 按首字母排序\n",
    "sorted_words = sorted(words)\n",
    "\n",
    "# 将去重并排序后的单词写入新文件\n",
    "with open('C:\\\\Users\\\\FullZero\\\\Desktop\\\\output3.txt', 'w') as f:\n",
    "    for word in sorted_words:\n",
    "        f.write(word + '\\n')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b709b550",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"with open('C:\\\\Users\\\\FullZero\\\\Desktop\\\\output.txt', 'r') as f:\n",
    "    lines = f.readlines()  # 读取所有行\n",
    "\n",
    "for i, line in enumerate(lines):\n",
    "    # 将每行的单词拆分成列表，并去除空白符\n",
    "    words = [word.strip() for word in line.split()]\n",
    "\n",
    "    # 创建新文件名，格式为 input_{行号}.txt\n",
    "    new_filename = f'C:\\\\Users\\\\FullZero\\\\Desktop\\\\fxxk\\\\data\\\\{i+1}.txt'\n",
    "\n",
    "    # 将该行单词写入新文件\n",
    "    with open(new_filename, 'w') as f:\n",
    "        for word in words:\n",
    "            f.write(word + '\\n')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b3e384c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中位数为： 268\n"
     ]
    }
   ],
   "source": [
    "\"\"\"#计算中位数\n",
    "with open('C:\\\\Users\\\\FullZero\\\\Desktop\\\\output.txt', 'r') as f:\n",
    "    lines = f.readlines()  # 读取所有行\n",
    "\n",
    "word_counts = []  # 存储每行词汇个数\n",
    "\n",
    "for line in lines:\n",
    "    words = line.split()  # 将一行词汇拆分成单个单词\n",
    "    word_count = len(words)  # 计算该行词汇个数\n",
    "    word_counts.append(word_count)  # 将词汇个数添加到列表中\n",
    "\n",
    "word_counts.sort()  # 将词汇个数从小到大排序\n",
    "\n",
    "if len(word_counts) % 2 == 0:\n",
    "    # 如果列表长度为偶数，则中位数为中间两个数的平均值\n",
    "    median = (word_counts[len(word_counts)//2] + word_counts[len(word_counts)//2-1]) / 2\n",
    "else:\n",
    "    # 如果列表长度为奇数，则中位数为中间一个数\n",
    "    median = word_counts[len(word_counts)//2]\n",
    "\n",
    "print(\"中位数为：\", median)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4f2ead8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import numpy as np\n",
    "\n",
    "# 生成随机行向量函数\n",
    "def random_vector(length, elem_length):\n",
    "    return np.random.randint(0, 10**elem_length, size=length)\n",
    "\n",
    "# 将行向量保存到文件中\n",
    "def save_vectors_to_file(vectors, file_path):\n",
    "    with open(file_path, 'w') as f:\n",
    "        for row in vectors:\n",
    "            row_str = ' '.join([str(x) for x in row])\n",
    "            f.write(row_str + '\\n')\n",
    "\n",
    "# 生成随机行向量列表\n",
    "num_vectors = 278\n",
    "vector_length = 300\n",
    "elem_length = 3\n",
    "vectors = [random_vector(vector_length, elem_length) for _ in range(num_vectors)]\n",
    "\n",
    "# 输出随机行向量列表\n",
    "#print(vectors)\n",
    "save_vectors_to_file(vectors, 'C:\\\\Users\\\\FullZero\\\\Desktop\\\\random_vectors.txt')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7807f540",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# 读取词汇文件\n",
    "with open('C:\\\\Users\\\\FullZero\\\\Desktop\\\\output3.txt', 'r') as f:\n",
    "    vocab = [line.strip() for line in f]\n",
    "\n",
    "# 构建Word2Vec模型的词汇表\n",
    "model = Word2Vec(vector_size=280, min_count=1)\n",
    "model.build_vocab([vocab])\n",
    "\n",
    "# 训练模型\n",
    "model.train(sentences, total_examples=len(sentences), epochs=10)\n",
    "\n",
    "# 保存模型\n",
    "model.save('C:\\\\Users\\\\FullZero\\\\Desktop\\\\word2vec.model')\n",
    "# 获取词汇到向量的映射\n",
    "#word_vectors = model.wv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac5efea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6.6039222e-04  2.7024983e-03 -2.4754389e-03 -3.5199106e-03\n",
      " -3.4766877e-04 -1.7882624e-03  1.5787231e-03  1.9730092e-03\n",
      " -3.1317987e-03 -2.9946715e-03 -2.3387901e-03 -1.2217087e-03\n",
      "  6.8288017e-04 -1.3105985e-03 -2.0806976e-03 -1.3400534e-03\n",
      " -3.2700470e-04  2.8645601e-03 -2.0132236e-04  4.7839625e-04\n",
      "  1.9107682e-03 -3.2543864e-03 -3.4784130e-03 -3.4133757e-03\n",
      " -1.8003460e-03 -1.3712326e-03 -8.1219629e-04 -1.5790369e-03\n",
      " -7.4251252e-04  2.0274844e-03 -1.1757812e-03  1.1888848e-03\n",
      " -3.0758786e-03 -1.4636853e-03 -3.2800138e-03 -3.0556337e-03\n",
      " -2.3002722e-03  6.8657356e-04 -1.5450945e-03  2.5750650e-03\n",
      "  1.3218058e-03 -2.4428125e-03  3.2829847e-03 -1.6203305e-03\n",
      " -2.5850700e-03 -5.6113303e-04  1.6945996e-03  2.1575477e-04\n",
      "  1.4523527e-03  3.4890473e-03 -1.4326951e-03  3.2443816e-03\n",
      "  3.1128142e-04  1.9750046e-03  2.6039868e-03  1.1063529e-03\n",
      "  3.1735701e-03 -3.2322509e-03 -3.8064932e-04  1.9441252e-03\n",
      "  3.3527911e-03  2.4869328e-03  7.8479730e-05 -7.6994300e-05\n",
      "  2.9398121e-03  1.3416712e-03  1.8573331e-03 -2.1146135e-03\n",
      " -7.2948846e-05 -3.2450818e-03 -3.6696365e-04 -3.1400565e-03\n",
      " -4.1485939e-04  3.2136701e-03 -2.6955064e-03  2.9081821e-03\n",
      " -2.3697820e-03  2.0964304e-03  1.5581322e-03 -2.4837763e-03\n",
      " -2.7542135e-03 -1.9492243e-03 -3.1503197e-03 -1.9308431e-03\n",
      "  4.4297619e-04 -3.0374071e-03 -3.5079753e-03  2.6675803e-03\n",
      "  1.3675472e-03 -1.3670146e-03  1.5103200e-03 -6.8963721e-04\n",
      "  3.2657390e-03  1.0037137e-03  1.4434623e-03 -3.3642845e-03\n",
      " -2.9016277e-03 -8.7702402e-04  2.2812127e-03 -2.9200043e-03\n",
      "  9.1898569e-04 -2.7808972e-04 -1.3466418e-03  1.7505778e-03\n",
      " -1.6148286e-03  2.9027122e-03 -2.9297357e-03  1.0078107e-03\n",
      " -7.1394315e-04 -2.4377534e-03  8.7140157e-04 -2.4291056e-03\n",
      "  1.8629023e-03 -2.9118513e-03 -2.6538831e-03 -2.1214711e-03\n",
      "  1.0895691e-03 -2.1120395e-04 -1.1747212e-03  2.0669389e-03\n",
      " -2.9808658e-03  9.1871835e-04 -2.9879527e-03 -6.1505777e-04\n",
      "  9.8766515e-04 -2.3091605e-03  5.7911489e-04  2.9782823e-03\n",
      "  6.4995245e-04 -1.2225002e-03 -1.6602793e-03 -1.9042841e-03\n",
      "  3.2832534e-03 -1.1265891e-03  1.5016258e-03  3.4327898e-03\n",
      "  2.2835855e-03 -2.4804508e-03  3.0796442e-03  9.9409523e-04\n",
      " -1.1761014e-03 -2.3498472e-03 -2.6700220e-03 -8.8682259e-04\n",
      "  1.9809860e-03 -2.2945213e-03 -3.5705380e-03 -2.9906153e-03\n",
      " -2.9344547e-03  2.7143597e-03 -2.5536572e-03 -7.2265352e-04\n",
      " -2.8960668e-03 -1.4388838e-03  7.9876225e-04  3.2063569e-03\n",
      "  8.2352414e-04  9.9028519e-04  2.9949141e-03 -3.0556880e-03\n",
      " -1.4123619e-04 -2.3308827e-03  1.5879533e-03 -6.7466754e-04\n",
      " -2.9196867e-04  9.9332503e-04 -3.0079894e-03 -2.6159329e-04\n",
      "  4.9816794e-04  1.6258913e-04  1.2078813e-03  8.3838915e-04\n",
      "  8.6821744e-04 -8.0680847e-04 -1.8075990e-03 -8.7301986e-04\n",
      " -3.5630001e-03 -2.5636971e-03  1.8905572e-04 -2.7033815e-04\n",
      " -1.1693895e-03  3.0138509e-03  1.3702546e-03  8.2851137e-04\n",
      "  9.0125290e-04 -1.0445165e-03  3.2465797e-04 -2.3716632e-03\n",
      " -1.4404696e-04  2.7714572e-03 -4.2394272e-04  4.8941543e-04\n",
      " -2.8380917e-03 -1.7356443e-03  2.9954033e-03  4.0472107e-04\n",
      "  3.7308334e-04  3.1997429e-03  1.4524332e-04  3.1637864e-03\n",
      " -7.8444608e-04  3.3648949e-04 -1.0250526e-03  3.2613831e-04\n",
      " -1.8264558e-03  4.5533522e-04  1.7168232e-03 -5.0299084e-05\n",
      "  1.4458461e-03  3.5214131e-03 -2.8020241e-03 -4.8296238e-04\n",
      " -1.2679121e-03  3.3576977e-03 -8.9160673e-04 -2.7616424e-03\n",
      " -1.4889240e-05  1.9570773e-03  3.5035696e-03 -1.4138554e-03\n",
      "  1.1352591e-03  3.0540619e-03  1.1045064e-03  1.8872351e-03\n",
      " -3.4537779e-03  2.6466348e-03  1.0076549e-04 -1.2686942e-03\n",
      "  2.9814376e-03 -3.2791751e-03  1.6072477e-03  1.6495815e-03\n",
      " -3.6854828e-05 -1.1465009e-03  2.5393690e-03  1.1166679e-03\n",
      "  3.4460102e-03 -2.4975729e-03 -1.3622706e-03 -1.0838483e-03\n",
      " -3.0786864e-04  2.4069335e-04 -2.9062016e-03  1.3911976e-03\n",
      "  3.1141640e-04  1.3973939e-03 -2.2331537e-03 -2.0700986e-03\n",
      "  2.4903577e-03  3.7383736e-04 -3.1383901e-03 -2.7402104e-03\n",
      " -1.4054030e-03 -2.9218269e-03  4.9399561e-04  1.3728337e-03\n",
      " -2.5709951e-04  4.2234149e-07 -1.8655743e-04  1.4930473e-03\n",
      " -2.8807409e-03  7.3774118e-04 -6.6482468e-04 -1.0077651e-03\n",
      " -1.3043804e-03 -7.3045277e-04 -3.1832627e-03 -3.0622040e-03\n",
      " -1.4633506e-03  1.9142078e-03 -3.2706831e-03 -1.8592949e-03\n",
      "  2.8831374e-03  1.4996869e-04 -3.3097586e-03  6.7982503e-05\n",
      " -2.4592297e-03 -2.9731793e-03 -8.3009433e-04 -4.6836989e-04]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# 加载已经训练好的Word2Vec模型\n",
    "model = Word2Vec.load('C:\\\\Users\\\\FullZero\\\\Desktop\\\\word2vec.model')\n",
    "\n",
    "# 获取词汇到向量的映射\n",
    "word_vectors = model.wv\n",
    "\n",
    "# 获取单个词汇的向量\n",
    "vector = word_vectors['connect']\n",
    "print(vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "00d13a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# 加载已经训练好的Word2Vec模型\n",
    "model = Word2Vec.load('C:\\\\Users\\\\FullZero\\\\Desktop\\\\word2vec.model')\n",
    "\n",
    "# 获取词汇到向量的映射\n",
    "word_vectors = model.wv\n",
    "\n",
    "# 将txt中的每个词汇转换为对应的向量值\n",
    "with open('C:\\\\Users\\\\FullZero\\\\Desktop\\\\fxxk\\\\data\\\\16.txt', 'r') as f_in, open('C:\\\\Users\\\\FullZero\\\\Desktop\\\\file.txt', 'w') as f_out:\n",
    "    for line in f_in:\n",
    "        words = line.strip().split()  # 分割一行词汇\n",
    "        vectors = [word_vectors[word] for word in words if word in word_vectors.index_to_key]  # 将每个词汇转换为向量值\n",
    "        if vectors:\n",
    "            vectors_str = '\\n'.join([' '.join(map(str, vec)) for vec in vectors]) + '\\n'  # 将向量值转换为字符串并按行拼接\n",
    "            f_out.write(vectors_str)  # 将向量值写入新文件\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4da0af9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\FullZero\\AppData\\Local\\Temp\\ipykernel_21452\\1524361363.py:34: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  vectors = np.array(vectors)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "#def dealone\n",
    "# 加载预训练模型\n",
    "    model = Word2Vec.load(\"C:\\\\Users\\\\FullZero\\\\Desktop\\\\word2vec.model\")\n",
    "\n",
    "# 定义词向量维度\n",
    "    EMBEDDING_DIM = 300\n",
    "\n",
    "# 读取文本文件\n",
    "    with open('C:\\\\Users\\\\FullZero\\\\Desktop\\\\fxxk\\\\data\\\\16.txt', 'r', encoding='utf-8') as f:\n",
    "        lines = f.read().splitlines()\n",
    "\n",
    "# 转换文本为词向量\n",
    "    vectors = []\n",
    "    for line in lines:\n",
    "        words = line.split()\n",
    "        vector_line = []\n",
    "        for word in words:\n",
    "            try:\n",
    "                vector = model.wv[word]\n",
    "            except KeyError:\n",
    "                vector = np.zeros(EMBEDDING_DIM)\n",
    "            vector_line.append(vector)\n",
    "        vectors.append(vector_line)\n",
    "\n",
    "# 将词向量转换为numpy数组，并将不足300行的用全零向量填充\n",
    "    num_rows = len(vectors)\n",
    "    if num_rows < 280:\n",
    "        zero_vector = np.zeros(EMBEDDING_DIM)\n",
    "        for _ in range(280 - num_rows):\n",
    "            vectors.append([zero_vector] * len(vectors[0]))\n",
    "\n",
    "    vectors = np.array(vectors)\n",
    "\n",
    "# 将numpy数组保存为文本文件\n",
    "    with open('C:\\\\Users\\\\FullZero\\\\Desktop\\\\file.txt', 'w', encoding='utf-8') as f:\n",
    "        for row in vectors:\n",
    "            row_str = \"\\t\".join([\",\".join(str(num) for num in vec) for vec in row])\n",
    "            f.write(row_str + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "972c4ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import os\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "def txt_to_vector(file_path, model_path, output_path, num_rows=280, vector_len=280):\n",
    "    # 加载预训练模型\n",
    "    model = Word2Vec.load(model_path)\n",
    "\n",
    "    # 读取文本文件\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.read().splitlines()\n",
    "\n",
    "    # 转换文本为词向量\n",
    "    vectors = []\n",
    "    for line in lines:\n",
    "        words = line.split()\n",
    "        vector_line = []\n",
    "        for word in words:\n",
    "            try:\n",
    "                vector = model.wv[word]\n",
    "            except KeyError:\n",
    "                vector = np.zeros(vector_len)\n",
    "            vector_line.append(vector)\n",
    "        vectors.append(vector_line)\n",
    "\n",
    "    # 将词向量转换为numpy数组，并将不足num_rows行的用全零向量填充\n",
    "    #num_rows = min(num_rows, len(vectors))\n",
    "    zero_vector = np.zeros(vector_len)\n",
    "    for _ in range(num_rows - len(vectors)):\n",
    "        vectors.append([zero_vector] * len(vectors[0]))\n",
    "\n",
    "    vectors = np.array(vectors)[:num_rows]\n",
    "\n",
    "    # 将numpy数组保存为文本文件\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for row in vectors:\n",
    "            row_str = \"\\t\".join([\",\".join(str(num) for num in vec) for vec in row])\n",
    "            f.write(row_str + \"\\n\")\n",
    "\n",
    "# 批量处理多个文件\n",
    "input_dir = 'C:\\\\Users\\\\FullZero\\\\Desktop\\\\fxxk\\\\data'\n",
    "output_dir = 'C:\\\\Users\\\\FullZero\\\\Desktop\\\\fxxk\\\\vectors'\n",
    "model_path = 'C:\\\\Users\\\\FullZero\\\\Desktop\\\\word2vec.model'\n",
    "for filename in os.listdir(input_dir):\n",
    "    input_path = os.path.join(input_dir, filename)\n",
    "    output_path = os.path.join(output_dir, filename)\n",
    "    txt_to_vector(input_path, model_path, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4830d24d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('C:\\\\Users\\\\FullZero\\\\Desktop\\\\dataset\\\\UNSW_NB15_training-set.csv')\n",
    "df = df.iloc[:, 1:]\n",
    "df.to_csv('C:\\\\Users\\\\FullZero\\\\Desktop\\\\dataset\\\\UNSW_NB15_training-set1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f2a8e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('C:\\\\Users\\\\FullZero\\\\Desktop\\\\dataset\\\\UNSW_NB15_testing-set.csv')\n",
    "data = data.drop(data.columns[0], axis=1)\n",
    "data.to_csv('C:\\\\Users\\\\FullZero\\\\Desktop\\\\dataset\\\\UNSW_NB15_testing-set1.csv', header=None, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b57e044e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                 Normal\n",
      "47910           Backdoor\n",
      "47931           Analysis\n",
      "48016            Fuzzers\n",
      "48812          Shellcode\n",
      "48865     Reconnaissance\n",
      "48931           Exploits\n",
      "48998                DoS\n",
      "49958              Worms\n",
      "117203           Generic\n",
      "Name: Normal, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "df = pd.read_csv('C:\\\\Users\\\\FullZero\\\\Desktop\\\\dataset\\\\UNSW_NB15_training-set1.csv')\n",
    "unique_values = df.iloc[:, 42].drop_duplicates()\n",
    "\n",
    "print(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b7719f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data from file\n",
    "data = pd.read_csv('C:\\\\Users\\\\FullZero\\\\Desktop\\\\dataset\\\\UNSW_NB15_training-set.csv')\n",
    "\n",
    "# Copy the first 20 rows of the data\n",
    "new_data = data.head(20).copy()\n",
    "\n",
    "# Save the new data to file\n",
    "new_data.to_csv('C:\\\\Users\\\\FullZero\\\\Desktop\\\\dataset\\\\new_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7c6737d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter id: 1\n",
      "Enter dur: 0.121478\n",
      "Enter proto: tcp\n",
      "Enter service: -\n",
      "Enter state: FIN\n",
      "Enter spkts: 6\n",
      "Enter dpkts: 4\n",
      "Enter sbytes: 258\n",
      "Enter dbytes: 172\n",
      "Enter rate: 74.08749\n",
      "Enter sttl: 252\n",
      "Enter dttl: 254\n",
      "Enter sload: 14158.94\n",
      "Enter dload: 8495.365\n",
      "Enter sloss: 0\n",
      "Enter dloss: 0\n",
      "Enter sinpkt: 24.2956\n",
      "Enter dinpkt: 8.375\n",
      "Enter sjit: 30.17755\n",
      "Enter djit: 11.8306\n",
      "Enter swin: 255\n",
      "Enter stcpb: 621772692\n",
      "Enter dtcpb: 2202533631\n",
      "Enter dwin: 255\n",
      "Enter tcprtt: 0\n",
      "Enter synack: 0\n",
      "Enter ackdat: 0\n",
      "Enter smean: 43\n",
      "Enter dmean: 43\n",
      "Enter trans_depth: 0\n",
      "Enter response_body_len: 0\n",
      "Enter ct_srv_src: 1\n",
      "Enter ct_state_ttl: 0\n",
      "Enter ct_dst_ltm: 1\n",
      "Enter ct_src_dport_ltm: 1\n",
      "Enter ct_dst_sport_ltm: 1\n",
      "Enter ct_dst_src_ltm: 1\n",
      "Enter is_ftp_login: 0\n",
      "Enter ct_ftp_cmd: 0\n",
      "Enter ct_flw_http_mthd: 0\n",
      "Enter ct_src_ltm: 1\n",
      "Enter ct_srv_dst: 1\n",
      "Enter is_sm_ips_ports: 0\n",
      "Enter attack_cat: \n",
      "Enter label: \n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "header = [\"id\", \"dur\", \"proto\", \"service\", \"state\", \"spkts\", \"dpkts\", \"sbytes\", \"dbytes\", \"rate\", \"sttl\", \"dttl\", \"sload\", \"dload\", \"sloss\", \"dloss\", \"sinpkt\", \"dinpkt\", \"sjit\", \"djit\", \"swin\", \"stcpb\", \"dtcpb\", \"dwin\", \"tcprtt\", \"synack\", \"ackdat\", \"smean\", \"dmean\", \"trans_depth\", \"response_body_len\", \"ct_srv_src\", \"ct_state_ttl\", \"ct_dst_ltm\", \"ct_src_dport_ltm\", \"ct_dst_sport_ltm\", \"ct_dst_src_ltm\", \"is_ftp_login\", \"ct_ftp_cmd\", \"ct_flw_http_mthd\", \"ct_src_ltm\", \"ct_srv_dst\", \"is_sm_ips_ports\", \"attack_cat\", \"label\"]\n",
    "\n",
    "data = []\n",
    "for column in header:\n",
    "    value = input(\"Enter {}: \".format(column))\n",
    "    data.append(value)\n",
    "\n",
    "with open(\"output.csv\", \"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(header)\n",
    "    writer.writerow(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d327c55e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
